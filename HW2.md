HW2 Q1
================

Saratoga house prices: Describing the price-modeling strategies for a local taxing authority:

    ##            (Intercept)                lotSize                    age 
    ##            28627.73165             9350.45188               47.54722 
    ##             livingArea             pctCollege               bedrooms 
    ##               91.86974              296.50809           -15630.71950 
    ##             fireplaces              bathrooms                  rooms 
    ##              985.06117            22006.97108             3259.11923 
    ## heatinghot water/steam        heatingelectric           fuelelectric 
    ##            -9429.79463            -3609.98574           -12094.12195 
    ##                fueloil           centralAirNo 
    ##            -8873.13971           -17112.81908

    ##            (Intercept)                lotSize                    age 
    ##            28627.73165             9350.45188               47.54722 
    ##             livingArea             pctCollege               bedrooms 
    ##               91.86974              296.50809           -15630.71950 
    ##             fireplaces              bathrooms                  rooms 
    ##              985.06117            22006.97108             3259.11923 
    ## heatinghot water/steam        heatingelectric           fuelelectric 
    ##            -9429.79463            -3609.98574           -12094.12195 
    ##                fueloil           centralAirNo 
    ##            -8873.13971           -17112.81908

    ## [1] 82524.47

    ## [1] 70199.39

    ## [1] 72806.49

    ##           V1       V2       V3
    ## 1   58392.24 57349.89 50367.95
    ## 2   63535.86 63152.34 56385.10
    ## 3   59069.38 58936.20 54345.47
    ## 4   59056.60 58633.11 55289.06
    ## 5   70305.72 69101.19 54009.29
    ## 6   62788.08 62331.66 56854.27
    ## 7   68302.92 68110.97 58687.90
    ## 8   74507.58 74160.78 58892.28
    ## 9   64394.29 63834.89 55435.73
    ## 10  70942.00 70176.58 58224.02
    ## 11  58983.85 59508.17 55195.99
    ## 12  57888.98 57304.52 50425.58
    ## 13  62596.00 64046.07 59275.96
    ## 14  75459.62 75909.35 64859.34
    ## 15  60250.75 59724.42 54291.31
    ## 16  71163.48 70403.42 62924.10
    ## 17  58588.39 57993.02 50963.78
    ## 18  67597.31 66861.35 55903.56
    ## 19  57911.57 57198.54 53328.93
    ## 20  64272.35 64211.16 58038.98
    ## 21  63482.31 61995.30 56112.14
    ## 22  61154.87 61139.56 55356.94
    ## 23  68285.25 66956.43 59407.99
    ## 24  69396.46 68628.56 55172.77
    ## 25  68169.72 67868.94 61297.10
    ## 26  63136.94 62854.31 56316.10
    ## 27  64940.89 64324.07 53677.45
    ## 28  58889.50 58997.97 52615.81
    ## 29  65695.91 64711.71 58986.12
    ## 30  67869.17 67089.15 59898.62
    ## 31  59949.55 61027.47 56132.83
    ## 32  82284.06 82230.72 69713.65
    ## 33  65104.33 65631.42 58909.60
    ## 34  63927.76 63901.23 60390.72
    ## 35  69390.98 68365.20 62748.15
    ## 36  66634.77 67186.29 61138.36
    ## 37  68127.46 67358.89 57014.00
    ## 38  70484.69 70500.43 63297.75
    ## 39  67017.02 66075.45 60989.51
    ## 40  60309.09 61095.43 53582.32
    ## 41  73973.19 74213.06 61163.34
    ## 42  66412.83 66206.79 56000.89
    ## 43  65311.12 64555.40 57271.72
    ## 44  71018.33 70169.36 60241.77
    ## 45  59170.90 58505.11 54629.29
    ## 46  68153.67 68261.50 62147.73
    ## 47  71334.98 70187.19 62019.69
    ## 48  60964.42 60246.27 55072.26
    ## 49  61987.75 61071.25 52190.34
    ## 50  69648.92 69792.68 62759.83
    ## 51  63342.83 62470.06 56936.10
    ## 52  62820.46 62249.85 53026.05
    ## 53  74069.68 74294.76 64107.99
    ## 54  63389.51 63670.14 57080.64
    ## 55  55723.40 55426.98 52243.67
    ## 56  71133.80 70438.09 63284.42
    ## 57  70398.74 69058.13 60632.65
    ## 58  69148.43 68208.97 58720.60
    ## 59  68714.57 67557.53 58521.23
    ## 60  67138.87 66079.42 62574.05
    ## 61  66397.06 66332.18 61856.84
    ## 62  73722.63 72517.92 61409.55
    ## 63  63952.97 64737.87 55230.25
    ## 64  64647.24 63772.02 56088.87
    ## 65  72008.52 71515.80 60966.64
    ## 66  59063.22 58304.86 53263.15
    ## 67  59055.39 58116.93 52159.08
    ## 68  68009.99 67595.59 57879.77
    ## 69  69317.13 69627.90 61861.34
    ## 70  62638.44 63018.53 56553.91
    ## 71  60275.15 58701.26 56616.66
    ## 72  69853.66 68417.78 58521.29
    ## 73  56367.02 56515.66 54183.78
    ## 74  69087.45 68691.51 61006.03
    ## 75  61666.07 61311.57 56836.48
    ## 76  67124.92 67085.62 61814.26
    ## 77  65112.40 64059.84 59206.81
    ## 78  65873.93 66870.83 58506.80
    ## 79  61084.14 61215.67 53074.24
    ## 80  66855.83 66219.99 54448.89
    ## 81  73677.48 73450.09 66565.41
    ## 82  66096.41 65805.65 57699.40
    ## 83  56132.39 55481.31 49754.60
    ## 84  65307.78 64561.48 57708.07
    ## 85  75938.61 75240.70 62525.00
    ## 86  64308.52 64428.08 52842.55
    ## 87  68635.86 68645.32 57606.27
    ## 88  69827.13 69017.87 62539.01
    ## 89  69082.38 68872.05 56964.00
    ## 90  59990.41 58830.25 53069.81
    ## 91  57501.27 56492.19 52569.50
    ## 92  72434.75 72237.03 58529.83
    ## 93  64034.06 63879.98 55545.34
    ## 94  66610.76 67120.86 58562.10
    ## 95  64387.46 63597.26 56112.31
    ## 96  74666.44 73702.31 63271.73
    ## 97  66590.34 65831.33 60363.58
    ## 98  62465.47 63046.60 55954.51
    ## 99  71790.95 71643.56 65866.56
    ## 100 67191.27 66961.94 55795.96

    ##       V1       V2       V3 
    ## 65828.97 65430.24 57784.11

Here is a hand-built model (lm\_hw) that outperforms the "medium" model considered in class. The lower RMSE indicates that lm\_hw outperforms lm\_medium and the lm\_boom models that we considered in class. The interaction between bedrooms and bathrooms seems to be an especially strong driver of house prices.

    ## [1] 63327.96 59141.33

Here is our KNN model using the same variables and standardized. Some are necessarily dropped because they are string variables.

    ## Warning in if (ntr < k) {: the condition has length > 1 and only the first
    ## element will be used

    ## Warning in if (k < 1) stop(gettextf("k = %d must be at least 1", k), domain
    ## = NA): the condition has length > 1 and only the first element will be used

![](data_mining_files/figure-markdown_github/unnamed-chunk-3-1.png)

    ## [1] 68475.14

    ## [1] 78

Above is a plot of RMSE versus K. K is on the x axis and RMSE is on the y axis. We see that the RMSE is minimized when k= 77. The RMSE in our linear model is lower than that of our KNN model likely due to dropping the string varaibles. The linear model seems to do the best looking at out of sample (we split into train and test to see this). The KNN model, if we would like to use it, performs best when we have k=77.



HW2 Q2
================

Part 1: Which radiologist is much more clinically conservative?

When radiologist gave recall the patient after seeing the mammograms when the patient doesn't need that, we think the radiologist is more clinically conservative. Fisrt step, in order to hold patient risk factors eauql,we use the whole data set to do the logistic regrssion, to set the model which can be uesd to judge the probability that the patient need to recall after her mammogram seen by the radiologist. Second step, we predict the result that whether a patient need to "recall" Third step, we compare the radiologist's judge and model's judge, to determine whether the radiologist is clinically conservative using confusion table.

    ## , , yhat = 0
    ## 
    ##                
    ## y                 0   1
    ##   radiologist13 169  29
    ##   radiologist34 180  17
    ##   radiologist66 161  37
    ##   radiologist89 159  38
    ##   radiologist95 170  27

Last, we calculate which radiologist's probabilty of giving 'recall' to a patient that doesn't need 'recall' is the highest.

``` r
29/(169+29)
```

    ## [1] 0.1464646

``` r
17/(180+17)
```

    ## [1] 0.08629442

``` r
37/(161+37)
```

    ## [1] 0.1868687

``` r
38/(159+38)
```

    ## [1] 0.1928934

``` r
27/(170+27)
```

    ## [1] 0.1370558

Thus, radiologist 89 is most clinically conservative compared to other radiologists.

Part 2

does the data suggest that they should be weighing some clinical risk factors more heavily than they currently are?

In order to show which model is much suitable, we calculate the RMSE and log likelihood in this question. And split the dataset into train set and test set.

    ##            V1        V2        V3        V4
    ## 1   0.1688635 0.1694837 0.1694483 0.1718647
    ## 2   0.1790044 0.1793082 0.1793792 0.1797724
    ## 3   0.1518040 0.1525585 0.1524993 0.1519802
    ## 4   0.1930810 0.1932687 0.1934355 0.1967107
    ## 5   0.1919151 0.1922523 0.1922636 0.1916664
    ## 6   0.1970836 0.1981867 0.1986849 0.1986145
    ## 7   0.1812899 0.1813555 0.1830403 0.1815912
    ## 8   0.2114968 0.2114554 0.2115437 0.2192399
    ## 9   0.1787220 0.1788866 0.1792796 0.1790718
    ## 10  0.1700805 0.1700230 0.1702153 0.1738495
    ## 11  0.1692561 0.1691909 0.1692887 0.1691406
    ## 12  0.2020506 0.2021068 0.2020356 0.2095208
    ## 13  0.1799147 0.1806427 0.1806778 0.1850656
    ## 14  0.1497687 0.1501201 0.1517155 0.1570281
    ## 15  0.1588471 0.1588335 0.1588245 0.1611787
    ## 16  0.1904745 0.1915206 0.1922200 0.1926127
    ## 17  0.1817985 0.1819103 0.1831156 0.1841064
    ## 18  0.2183136 0.2186390 0.2185172 0.2198857
    ## 19  0.1884953 0.1884820 0.1885696 0.1938544
    ## 20  0.2091953 0.2096703 0.2095999 0.2098465
    ## 21  0.2223553 0.2224805 0.2224110 0.2252473
    ## 22  0.2303653 0.2303563 0.2311988 0.2330687
    ## 23  0.1316887 0.1316073 0.1315337 0.1362377
    ## 24  0.1876634 0.1876037 0.1899996 0.1870774
    ## 25  0.1715431 0.1724211 0.1731306 0.1747894
    ## 26  0.2172910 0.2177574 0.2178212 0.2165758
    ## 27  0.2022649 0.2024543 0.2029065 0.2045779
    ## 28  0.1940295 0.1946061 0.1945493 0.1943685
    ## 29  0.2057759 0.2057587 0.2057889 0.2077759
    ## 30  0.1661598 0.1668108 0.1667306 0.1678237
    ## 31  0.1899732 0.1899117 0.1898662 0.1918290
    ## 32  0.2238708 0.2238746 0.2237986 0.2238344
    ## 33  0.1929663 0.1929333 0.1932332 0.1932217
    ## 34  0.1230702 0.1229724 0.1228421 0.1231200
    ## 35  0.1708847 0.1713334 0.1716973 0.1754630
    ## 36  0.2340733 0.2342358 0.2357749 0.2337153
    ## 37  0.1391181 0.1395238 0.1395816 0.1381502
    ## 38  0.1546250 0.1552277 0.1551551 0.1561065
    ## 39  0.1483410 0.1486902 0.1487143 0.1488363
    ## 40  0.1922286 0.1924688 0.1925592 0.1941150
    ## 41  0.1856410 0.1855862 0.1862187 0.1904070
    ## 42  0.1642650 0.1642735 0.1653765 0.1708589
    ## 43  0.1709603 0.1710599 0.1714840 0.1747970
    ## 44  0.1560426 0.1563797 0.1566846 0.1567993
    ## 45  0.2278923 0.2280323 0.2288942 0.2286493
    ## 46  0.1889749 0.1892073 0.1895036 0.1922918
    ## 47  0.1734946 0.1736398 0.1736206 0.1719771
    ## 48  0.2018446 0.2023802 0.2023568 0.2028716
    ## 49  0.2037446 0.2045498 0.2044830 0.2063898
    ## 50  0.2211887 0.2215673 0.2222227 0.2233468
    ## 51  0.2285020 0.2286562 0.2287791 0.2286213
    ## 52  0.2317923 0.2318739 0.2333473 0.2380726
    ## 53  0.1892919 0.1892487 0.1891874 0.1921820
    ## 54  0.1436112 0.1439654 0.1474974 0.1484962
    ## 55  0.2217067 0.2217537 0.2216714 0.2267932
    ## 56  0.1613591 0.1620267 0.1622824 0.1587936
    ## 57  0.2012737 0.2014942 0.2017272 0.2035857
    ## 58  0.2152878 0.2152749 0.2151990 0.2200474
    ## 59  0.2099131 0.2102363 0.2105374 0.2113521
    ## 60  0.1907404 0.1906841 0.1906187 0.1941617
    ## 61  0.1980860 0.1986143 0.2026696 0.1974829
    ## 62  0.2195922 0.2196025 0.2195264 0.2205095
    ## 63  0.1571978 0.1571968 0.1572278 0.1567947
    ## 64  0.2092586 0.2095372 0.2095484 0.2142563
    ## 65  0.1619655 0.1620621 0.1646098 0.1655083
    ## 66  0.1922286 0.1928349 0.1941255 0.1954111
    ## 67  0.1709603 0.1713919 0.1713358 0.1745149
    ## 68  0.1751140 0.1751400 0.1751769 0.1732964
    ## 69  0.2066694 0.2080808 0.2099085 0.2085013
    ## 70  0.1971709 0.1971078 0.2009477 0.1961239
    ## 71  0.1144747 0.1150195 0.1149143 0.1190340
    ## 72  0.2067146 0.2067070 0.2068824 0.2083543
    ## 73  0.1402208 0.1401767 0.1401760 0.1457593
    ## 74  0.1975771 0.1976932 0.2025995 0.1984995
    ## 75  0.1541820 0.1543304 0.1543401 0.1552037
    ## 76  0.1796004 0.1795355 0.1795939 0.1783457
    ## 77  0.1998801 0.1999252 0.2016519 0.2057640
    ## 78  0.1902189 0.1909870 0.1924005 0.1960697
    ## 79  0.1749620 0.1749256 0.1749025 0.1782812
    ## 80  0.1776883 0.1776592 0.1776300 0.1805863
    ## 81  0.1596883 0.1599262 0.1604488 0.1641811
    ## 82  0.1180059 0.1179093 0.1179984 0.1225465
    ## 83  0.1560426 0.1560167 0.1564949 0.1553063
    ## 84  0.1940295 0.1943021 0.1946634 0.1967939
    ## 85  0.1454727 0.1457683 0.1456892 0.1469646
    ## 86  0.1997110 0.1996559 0.1996397 0.2063076
    ## 87  0.1560426 0.1560609 0.1569493 0.1586349
    ## 88  0.1567425 0.1567732 0.1566878 0.1588165
    ## 89  0.1764180 0.1767905 0.1766989 0.1743924
    ## 90  0.1833087 0.1832520 0.1833438 0.1814104
    ## 91  0.1230702 0.1229784 0.1228806 0.1280251
    ## 92  0.1581589 0.1585577 0.1584789 0.1566817
    ## 93  0.2123402 0.2128217 0.2138984 0.2156099
    ## 94  0.1665239 0.1668408 0.1668587 0.1677154
    ## 95  0.1736765 0.1737923 0.1739245 0.1770411
    ## 96  0.1847688 0.1850324 0.1852395 0.1889303
    ## 97  0.1977008 0.1977927 0.1977485 0.1980037
    ## 98  0.2319934 0.2321090 0.2324016 0.2332071
    ## 99  0.1876789 0.1876280 0.1875496 0.1875093
    ## 100 0.1884953 0.1886208 0.1924096 0.1914853

    ##        V1        V2        V3        V4 
    ## 0.1835090 0.1837397 0.1842551 0.1857089

According to the RMSE, we found that the model only link the recall and cancer is the most suitable to predict cancer status.

    ##            V1        V2        V3        V4
    ## 1   -3.982324 -3.980165 -4.011343 -4.047815
    ## 2   -4.466497 -4.475813 -4.504362 -4.467612
    ## 3   -3.279591 -3.290002 -3.301696 -3.341414
    ## 4   -5.733250 -5.760668 -5.758312 -5.833442
    ## 5   -2.635971 -2.638581 -2.645481 -2.650663
    ## 6   -6.402626 -6.411929 -6.408225 -6.357038
    ## 7   -3.517582 -3.519463 -3.515275 -3.557978
    ## 8   -3.150612 -3.161401 -3.175277 -3.120454
    ## 9   -4.576295 -4.576845 -4.660240 -4.602011
    ## 10  -5.317139 -5.330357 -5.355847 -5.291281
    ## 11  -3.748735 -3.753496 -3.778785 -3.746832
    ## 12  -3.573634 -3.601322 -3.843966 -3.769466
    ## 13  -3.564049 -3.564310 -3.563653 -3.715642
    ## 14  -3.083552 -3.081242 -3.080301 -2.996559
    ## 15  -2.368978 -2.370483 -2.393220 -2.372967
    ## 16  -2.651058 -2.655715 -2.654349 -2.691520
    ## 17  -3.199929 -3.241125 -3.244871 -3.394158
    ## 18  -4.208998 -4.209512 -4.217378 -4.384981
    ## 19  -3.785158 -3.799586 -3.801067 -3.843458
    ## 20  -3.188375 -3.195592 -3.208250 -3.134456
    ## 21  -3.102996 -3.101670 -3.158943 -3.149131
    ## 22  -3.279591 -3.278009 -3.277649 -3.265501
    ## 23  -1.607336 -1.605409 -1.647500 -1.725337
    ## 24  -4.485354 -4.486172 -4.482634 -4.635480
    ## 25  -4.606932 -4.628295 -4.704801 -4.669360
    ## 26  -5.169983 -5.172386 -5.173591 -5.246314
    ## 27  -2.479151 -2.511469 -2.513488 -2.501329
    ## 28  -5.468459 -5.534486 -5.539502 -5.647915
    ## 29  -2.130770 -2.133476 -2.131769 -2.219885
    ## 30  -4.182954 -4.180635 -4.207287 -4.303893
    ## 31  -3.806046 -3.806061 -3.802856 -3.870660
    ## 32  -2.849347 -2.851964 -2.875482 -2.965629
    ## 33  -5.883888 -5.888282 -5.893175 -5.999843
    ## 34  -2.351229 -2.349312 -2.376078 -2.368857
    ## 35  -2.776650 -2.775771 -2.772604 -2.850399
    ## 36  -3.222309 -3.245170 -3.243623 -3.369434
    ## 37  -3.476509 -3.488191 -3.485607 -3.716382
    ## 38  -2.656614 -2.660036 -2.667548 -2.786156
    ## 39  -3.499752 -3.508236 -3.506014 -3.569959
    ## 40  -2.835329 -2.834416 -2.868663 -2.836042
    ## 41  -2.676020 -2.677962 -2.692880 -2.724138
    ## 42  -4.405994 -4.409986 -4.407357 -4.598413
    ## 43  -2.863865 -2.865187 -2.918972 -2.833344
    ## 44  -3.521586 -3.545902 -3.770497 -3.761822
    ## 45  -3.510553 -3.512100 -3.537980 -3.616290
    ## 46  -3.942251 -3.946854 -3.944789 -3.947465
    ## 47  -4.679392 -4.685578 -4.739949 -4.807318
    ## 48  -4.057532 -4.055363 -4.070494 -3.971439
    ## 49  -3.836756 -3.837485 -3.846960 -4.059156
    ## 50  -4.686085 -4.685162 -4.732359 -4.904783
    ## 51  -4.689111 -4.689437 -4.687672 -4.768286
    ## 52  -2.084486 -2.085592 -2.114260 -2.267480
    ## 53  -2.220006 -2.226404 -2.224074 -2.215689
    ## 54  -2.429988 -2.457482 -2.614413 -2.616085
    ## 55  -2.434049 -2.432389 -2.432090 -2.463596
    ## 56  -3.448187 -3.446169 -3.444407 -3.468440
    ## 57  -2.964888 -2.965857 -2.996650 -2.992191
    ## 58  -3.211923 -3.209649 -3.209646 -3.212433
    ## 59  -2.903299 -2.918099 -2.924112 -3.045053
    ## 60  -4.057532 -4.055309 -4.081023 -4.013869
    ## 61  -4.325612 -4.375813 -4.424410 -4.667392
    ## 62  -3.166524 -3.164972 -3.178327 -3.163901
    ## 63  -2.413916 -2.412237 -2.409300 -2.534825
    ## 64  -2.170875 -2.169580 -2.202489 -2.241641
    ## 65  -3.211923 -3.212567 -3.249161 -3.255808
    ## 66  -4.269552 -4.276178 -4.273930 -4.216900
    ## 67  -3.492755 -3.494328 -3.516272 -3.539440
    ## 68  -2.564982 -2.571145 -2.584057 -2.595880
    ## 69  -4.216811 -4.215725 -4.227112 -4.187778
    ## 70  -3.583619 -3.594861 -3.604770 -3.589811
    ## 71  -3.650141 -3.649516 -3.649616 -3.764351
    ## 72  -3.362743 -3.365725 -3.363977 -3.406122
    ## 73  -4.805034 -4.817146 -4.830905 -4.898159
    ## 74  -4.814125 -4.835161 -4.870811 -4.912842
    ## 75  -2.143126 -2.165550 -2.162213 -2.313873
    ## 76  -3.865610 -3.878205 -3.885573 -3.896532
    ## 77  -2.189418 -2.188122 -2.190462 -2.161788
    ## 78  -3.171895 -3.169590 -3.267400 -3.211221
    ## 79  -4.399871 -4.398582 -4.397791 -4.437098
    ## 80  -4.492091 -4.491792 -4.526653 -4.430167
    ## 81  -3.057231 -3.054873 -3.136819 -3.147857
    ## 82  -3.983385 -3.985907 -3.982875 -3.953027
    ## 83  -4.388450 -4.387215 -4.386752 -4.565994
    ## 84  -3.260097 -3.269627 -3.266111 -3.287109
    ## 85  -3.517582 -3.582131 -3.593121 -3.611792
    ## 86  -5.782668 -5.790894 -5.787915 -5.781377
    ## 87  -4.047891 -4.051431 -4.049492 -4.219403
    ## 88  -2.878901 -2.881655 -2.930629 -2.886235
    ## 89  -4.325612 -4.348290 -4.345118 -4.342543
    ## 90  -4.340258 -4.338309 -4.337160 -4.376805
    ## 91  -2.770044 -2.786827 -2.783954 -2.720802
    ## 92  -1.954071 -1.951697 -2.027739 -2.129271
    ## 93  -4.913386 -4.912891 -4.988640 -4.976147
    ## 94  -4.998664 -5.031168 -5.032740 -4.995896
    ## 95  -3.349141 -3.385261 -3.407202 -3.386708
    ## 96  -3.146245 -3.149192 -3.160418 -3.069895
    ## 97  -4.309563 -4.314023 -4.358403 -4.476081
    ## 98  -3.113334 -3.117468 -3.115911 -3.212060
    ## 99  -3.957195 -3.958538 -4.213319 -4.063642
    ## 100 -3.319003 -3.331526 -3.335906 -3.470742

    ##        V1        V2        V3        V4 
    ## -3.606524 -3.614407 -3.638967 -3.674035

According to the RSS, we also found that the model only link the recall and cancer is the most suitable to predict cancer status.

Since the radiologist's opinion to 'recall' or not, is depended on the cancer status, thus if we include more covariates, there will be multicollinearity in the model and cause the result deviance bigger.



HW2 Q3
================

Predicting when articles go viral
---------------------------------

In this question, we use regression and classification to predict when articals go viral.


</br>

### 1. Baseline model

We always predict that the artical will go viral, since the number of viral articals is larger than the number of non-viral articals.

    ## y
    ##     0     1 
    ## 18490 21154


</br> The average confusion matrix for this baseline model is

    ##            y=0     y=1
    ## yhat=0    0.00 3691.12
    ## yhat=1    0.00 4237.88

    ## [1] 0.4655215


</br> And the average overall error rate is

    ## [1] 0.4655215

By construction of the baseline model, true positive and false positive rate are both 0.


 </br>

 </br>

### 2. Regression & threshold


 </br>

#### 1) Linear regression


 </br>
Selecting the variables and applying linear regression with linear effect only, we get overall error rate around 46.0%, which is nearly the same as the baseline model. Selecting the variables and applying regression with quadratic effect, we get overall error rate around 43%, which is slightly better than the baseline model.


</br>

#### 2) KNN


 </br>

We first try a grid of k from 1 to 101, calculate their average error rate in 5 trials, and select an optimal k. We select k=61 in this case.


</br>

![](/Rplot01.png)

 </br>
Then we get the confusion matrix, overall error rate, TPR and FPR by averaging across 100 rounds. The average overall error rate is around 37.5%, which is better than linear regression model.


</br>

Confusion matrix:

    ##           y=0    y=1
    ## yhat=0 2169.4 1545.9
    ## yhat=1 1445.7 2768.0


</br>

Overall error rate =

    ## [1] 0.3772985


</br> TPR =

    ## [1] 0.6569049


</br> FPR =

    ## [1] 0.4160902


</br>

</br>

### 3. Threshold & classification


</br>

We first categorize the articals into viral and non-viral using 1400 shares as threshold. Then we use logit regression to do the classification. We used similar variable selection as linear regression model. The confusion matrix, overall error rate, TPR and FPR averaging across 100 rounds are as below:


</br>


</br>


</br>

Confusion matrix:

    ##            y=0     y=1
    ## yhat=0 2048.54 1652.16
    ## yhat=1 1236.40 2991.90


</br>

Overall error rate =

    ## [1] 0.3643032


</br> TPR =

    ## [1] 0.7075893


</br> FPR =

    ## [1] 0.4464453


</br> The average overall error rate is around 36%, which is better than regression & threshold method. The reason might be that by first classifying the y using threshold, the model are less prone to outliers with extreme number of shares.
