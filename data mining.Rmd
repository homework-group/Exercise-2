---
title: "Homework 2- Saratoga House Prices"
output: github_document
---

```{r setup, include=FALSE}

```
 Describing the price-modeling strategies for a local taxing authority: 

```{r}
library(tidyverse)
library(mosaic)
data(SaratogaHouses)

summary(SaratogaHouses)

# Baseline model
lm_small = lm(price ~ bedrooms + bathrooms + lotSize, data=SaratogaHouses)

# 11 main effects
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=SaratogaHouses)

# Sometimes it's easier to name the variables we want to leave out
# The command below yields exactly the same model.
# the dot (.) means "all variables not named"
# the minus (-) means "exclude this variable"
lm_medium2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=SaratogaHouses)

coef(lm_medium)
coef(lm_medium2)

# All interactions
# the ()^2 says "include all pairwise interactions"
lm_big = lm(price ~ (. - sewer - waterfront - landValue - newConstruction)^2, data=SaratogaHouses)


####
# Compare out-of-sample predictive performance
####

# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]

# Fit to the training data
lm1 = lm(price ~ lotSize + bedrooms + bathrooms, data=saratoga_train)
lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
lm3 = lm(price ~ (. - sewer - waterfront - landValue - newConstruction)^2, data=saratoga_train)

# Predictions out of sample
yhat_test1 = predict(lm1, saratoga_test)
yhat_test2 = predict(lm2, saratoga_test)
yhat_test3 = predict(lm3, saratoga_test)

rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}

# Root mean-squared prediction error
rmse(saratoga_test$price, yhat_test1)
rmse(saratoga_test$price, yhat_test2)
rmse(saratoga_test$price, yhat_test3)


# easy averaging over train/test splits
library(mosaic)

rmse_vals = do(100)*{
  
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  # fit to this training set
  lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  
  lm_boom = lm(price ~ lotSize + age + pctCollege + 
                 fireplaces + rooms + heating + fuel + centralAir +
                 bedrooms*rooms + bathrooms*rooms + 
                 bathrooms*livingArea, data=saratoga_train)
  
  lm_biggerboom = lm(price ~ lotSize + landValue + waterfront + newConstruction + bedrooms*bathrooms + heating + fuel + pctCollege + rooms*bedrooms + rooms*bathrooms + rooms*heating + livingArea, data=saratoga_train)
  
  
  # predict on this testing set
  yhat_test2 = predict(lm2, saratoga_test)
  yhat_testboom = predict(lm_boom, saratoga_test)
  yhat_testbiggerboom = predict(lm_biggerboom, saratoga_test)
  c(rmse(saratoga_test$price, yhat_test2),
    rmse(saratoga_test$price, yhat_testboom),
    rmse(saratoga_test$price, yhat_testbiggerboom))
}

rmse_vals
colMeans(rmse_vals)
```
Here is a hand-built model (lm_hw) that outperforms the "medium" model considered in class. The lower RMSE indicates that lm_hw outperforms lm_medium and the lm_boom models that we considered in class. The interaction between bedrooms and bathrooms seems to be an especially strong driver of house prices. 

```{r }


# re-split into train and test cases
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]

# fit to this training set
lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)

lm_hw = lm(price ~ lotSize + landValue + waterfront + newConstruction + bedrooms*bathrooms + heating + fuel + pctCollege + rooms*bathrooms + livingArea, data=saratoga_train)


# predict on this testing set
yhat_test2 = predict(lm2, saratoga_test)
yhat_testhw = predict(lm_hw, saratoga_test)
c(rmse(saratoga_test$price, yhat_test2),rmse(saratoga_test$price, yhat_testhw))
```

Here is our KNN model using the same variables and standardized. Some are necessarily dropped because they are string variables.  

```{r, echo=FALSE}

library(mosaic)
library(tidyverse)
library(FNN)


# create a train/test split
N = nrow(SaratogaHouses)
N_train = floor(0.8*N)
train_ind = sample.int(N, N_train, replace=FALSE)

ST_train = SaratogaHouses[train_ind,]
ST_test = SaratogaHouses[-train_ind,]

y_train_ST = ST_train$price
X_train_ST = data.frame(lotSize = ST_train$lotSize, landValue = ST_train$landValue, bedrooms_bathrooms = ST_train$bedrooms*ST_train$bathrooms,
                        pctCollege = ST_train$pctCollege,
                        rooms_bathrooms = ST_train$rooms*ST_train$bathrooms, 
                        livingArea = ST_train$livingArea)
y_test_ST = ST_test$price
X_test_ST = data.frame(lotSize = ST_test$lotSize, landValue = ST_test$landValue,
                       bedrooms_bathrooms = ST_test$bedrooms*ST_test$bathrooms,
                       pctCollege = ST_test$pctCollege,
                       rooms_bathrooms = ST_test$rooms*ST_test$bathrooms, 
                       livingArea = ST_test$livingArea)

rmse = function(y, ypred) {
  sqrt(mean((y-ypred)^2))
}


library(foreach)

# scale the training set features
scale_factors = apply(X_train_ST, 2, sd)
X_train_sc = scale(X_train_ST, scale=scale_factors)

# scale the test set features using the same scale factors
X_test_sc = scale(X_test_ST, scale=scale_factors)

k_grid = unique(round(exp(seq(log(N_train), log(2), length=100))))
rmse_grid_out = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knn.reg(X_train_ST, X_test_ST, y_train_ST, k = k)
  rmse(y_test_ST, knn_model$pred)
}


rmse_grid_out = data.frame(K = k_grid, RMSE = rmse_grid_out)
ggplot(rmse_grid_out)+
  geom_line(rmse_grid_out,mapping = aes(x=rmse_grid_out[,1],y=rmse_grid_out[,2]))
min(rmse_grid_out[,2])
which.min(rmse_grid_out[,2])

```
Above is a plot of RMSE versus K. K is on the x axis and RMSE is on the y axis. We see that the RMSE is minimized when k about equal to 77. The RMSE in our linear model is lower than that of our KNN model likely due to dropping the string varaibles. The linear model seems to do the best looking at out of sample (we split into train and test to see this). The KNN model, if we would like to use it, performs best when we have k=77. 



title: "Question 2"
output: github_document
---
Part 1:
Which radiologist is much more clinically conservative?
```{r,include=FALSE}
library(tidyverse)
library(mosaic)
library(nnet)
setwd("~/Desktop/")
brca <- read.csv("brca.csv")
```

When radiologist gave recall the patient after seeing the mammograms when the patient doesn't need that, we think the radiologist is more clinically conservative.
Fisrt step, in order to hold patient risk factors eauql,we use the whole data set to do the logistic regrssion, to set the model which can be uesd to judge the probability that the patient need to recall after her mammogram seen by the radiologist.
Second step, we predict the result that whether a patient need to "recall"
Third step, we compare the radiologist's judge and model's judge, to determine whether the radiologist is clinically conservative.
```{r,echo=FALSE}
logit1 = glm(recall~ ., data = brca)
phat_logit = predict(logit1,brca,type = 'response')
yhat_logit1 = ifelse(phat_logit > 0.5, 1, 0)
confusion_out_logit = table(y= brca$radiologist,brca$recall, yhat = yhat_logit1)
confusion_out_logit
```
Last, we calculate which radiologist's probabilty of giving 'recall' to a patient that doesn't need 'recall' is the highest.
```{r}
25/(165+25)
14/(177+14)
33/(157+33)
22/(168+22)
```
Thus, radiologist66 and radiologist89 is more clinically conservative compared to other radiologists.

Part 2
In order to show which model is much suitable, we calculate the RMSE and log likelihood in this question. And split the dataset into train set and test set.
```{r,echo=FALSE}
rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
repN=100
rmse_vals = do(100)*{
  n = nrow(brca)
  n_train = round(0.8*n)  
  n_test = n - n_train
  
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  train_cases
  test_cases
  
  brca_train = brca[train_cases,]
  brca_test = brca[test_cases,]
  logit2_fit = glm(cancer ~ recall,data = brca_train)
  logit3_fit = glm(cancer ~ recall + history, data=brca_train)
  logit4_fit = glm(cancer ~ recall + history + recall*history, data=brca_train)
  logit5_fit = glm(cancer ~., data = brca_train)
  yhat_test1 = predict(logit2_fit, brca_test)
  yhat_test2 = predict(logit3_fit, brca_test)
  yhat_test3 = predict(logit4_fit, brca_test)
  yhat_test4 = predict(logit5_fit, brca_test)
  
  c(rmse(brca_test$cancer, yhat_test1),
    rmse(brca_test$cancer, yhat_test2),
    rmse(brca_test$cancer, yhat_test3),
    rmse(brca_test$cancer, yhat_test4))
  
 }
rmse_vals
colMeans(rmse_vals)
```
According to the RMSE, we found that the model only link the recall and cancer is the most suitable to predict cancer status.
```{r,echo=FALSE}
rss = function(y, yhat) {
  sum( (y - yhat)^2 ) 
}
rss_vals = do(100)*{
 
  n = nrow(brca)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  train_cases
  test_cases
  
  brca_train = brca[train_cases,]
  brca_test = brca[test_cases,]
  #set the fit model using the train data set  
  logit2_fit = glm(cancer ~ recall,data = brca_train)
  logit3_fit = glm(cancer ~ recall + history, data=brca_train)
  logit4_fit = glm(cancer ~ recall + history + recall*history, data=brca_train)
  logit5_fit = glm(cancer ~., data = brca_train)
  
  yhat_test1 = predict(logit2_fit, brca_test)
  yhat_test2 = predict(logit3_fit, brca_test)
  yhat_test3 = predict(logit4_fit, brca_test)
  yhat_test4 = predict(logit5_fit, brca_test)
  
  c(rss(brca_test$cancer, yhat_test1),
    rss(brca_test$cancer, yhat_test2),
    rss(brca_test$cancer, yhat_test3),
    rss(brca_test$cancer, yhat_test4))
  
  
}
rss_vals*-1/2
colMeans(rss_vals*-1/2)
```
According to the RSS, we also found that the model only link the recall and cancer is the most suitable to predict cancer status.

Since the radiologist's opinion to 'recall' or not, is depended on the cancer status, thus if we include more covariates, there will be multicollinearity in the model and cause the result deviance bigger.


---
title: "HW2 Q3"
output: github_document
---


```{r setup, include=FALSE}
rm(list=ls())
library(rmarkdown)
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
library(mosaic)
library(foreach)
setwd("~/Desktop/statistical learning/code/data")
online = read.csv("online_news.csv") 
online$url<-NULL
```



## Predicting when articles go viral

In this question, we use regression and classification to predict when articals go viral.

&NewLine;

</br>


```{r, include=FALSE}
# 0. Preparation: train-test split
n = nrow(online)
n_train = round(0.8*n)
n_test = n - n_train
X = online
y = ifelse(online$shares >= 1400, 1, 0)
```


###1. Baseline model

 We always predict that the artical will go viral, since the number of viral articals is larger than the number of non-viral articals.
 
  
```{r}
table(y) 
```


```{r, warning=FALSE, message=FALSE}
repN=100
conf11 <- rep(0,repN)
conf12 <- rep(0,repN)
conf21 <- rep(0,repN)
conf22 <- rep(0,repN)
for (i in 1:repN){
  train_ind = sample.int(n, n_train)
  y_train = y[train_ind]
  y_test = y[-train_ind]
  base_pred = rep(1, n_test)
  
  # Average confusion matrix
  conf12[i] <- table(y = y_test, yhat = base_pred)[1,1]
  conf22[i] <- table(y = y_test, yhat = base_pred)[2,1]
}
confusion_matrix <- matrix(c(mean(conf11),mean(conf12),mean(conf21),mean(conf22)),ncol=2,byrow=TRUE)
colnames(confusion_matrix) <- c("y=0","y=1")
rownames(confusion_matrix) <- c("yhat=0","yhat=1")
confusion_matrix <- as.table(confusion_matrix)
overall_error <- (confusion_matrix[1,2]+confusion_matrix[2,1])/length(y_test)
true_positive_rate <-  confusion_matrix[2,2] / (confusion_matrix[2,1]+confusion_matrix[2,2])
false_positive_rate <-  confusion_matrix[1,2] / (confusion_matrix[1,1]+confusion_matrix[1,2])
```

&NewLine;

</br>
The average confusion matrix for this baseline model is


```{r, include=TRUE}
confusion_matrix
overall_error
```

&NewLine;

</br>
And the average overall error rate is

```{r, include=TRUE}
overall_error
```
By construction of the baseline model, true positive and false positive rate are both 0.
 
  
  
&NewLine;
</br>  
&NewLine;
</br>

###2. Regression & threshold
&NewLine;
</br>  

#### 1) Linear regression

&NewLine;
</br>  
Selecting the variables and applying linear regression with linear effect only, we get overall error rate around 46.0%, which is nearly the same as the baseline model.
Selecting the variables and applying regression with quadratic effect, we get overall error rate around 43%, which is slightly better than the baseline model.

```{r, warning=FALSE, message=FALSE}
train_ind = sample.int(n, n_train)
  y_train = y[train_ind]
  y_test = y[-train_ind]
  X_train = X[train_ind,]
  X_test = X[-train_ind,]
  
lm1 = lm(shares ~ . - weekday_is_sunday - is_weekend, data=X_train)
lm2 = lm(shares ~ n_tokens_title  + num_self_hrefs + num_imgs +
           average_token_length + num_keywords + 
           data_channel_is_lifestyle + data_channel_is_entertainment+data_channel_is_bus +data_channel_is_socmed +
           data_channel_is_tech + data_channel_is_world +
           self_reference_min_shares + is_weekend + avg_negative_polarity , data=X_train)
lm3 = lm(shares ~ (.)^2, data=X_train)   # this performs the best
lm4 = lm(shares ~ n_tokens_title + num_hrefs + num_self_hrefs + num_imgs +
           average_token_length + num_keywords + 
           data_channel_is_lifestyle + data_channel_is_entertainment+data_channel_is_bus+
           data_channel_is_socmed + data_channel_is_tech + data_channel_is_world +
           self_reference_min_shares + is_weekend + avg_negative_polarity+
           
           n_tokens_title*self_reference_min_shares + num_hrefs*data_channel_is_tech +
           num_self_hrefs*self_reference_min_shares + average_token_length*self_reference_min_shares +
           data_channel_is_bus*avg_negative_polarity+data_channel_is_socmed*self_reference_min_shares +
           data_channel_is_tech*self_reference_min_shares +
           self_reference_min_shares*avg_negative_polarity
         , data=X_train)
```


```{r, warning=FALSE, message=FALSE}
# Average confusion matrix
for (i in 1:repN){
  train_ind = sample.int(n, n_train)
  y_train = y[train_ind]
  y_test = y[-train_ind]
  X_train = X[train_ind,]
  X_test = X[-train_ind,]
  
  lm = lm(shares ~ (.)^2, data=X_train)
  lm_share = predict(lm, X_test)
  lm_yhat = ifelse(lm_share >= 1400, 1, 0)
  
  # Average confusion matrix
  conf11[i] <- table(y = y_test, yhat = lm_yhat)[1,1]
  conf12[i] <- table(y = y_test, yhat = lm_yhat)[1,2]
  conf21[i] <- table(y = y_test, yhat = lm_yhat)[2,1]
  conf22[i] <- table(y = y_test, yhat = lm_yhat)[2,2]
}
confusion_matrix <- matrix(c(mean(conf11),mean(conf12),mean(conf21),mean(conf22)),ncol=2,byrow=TRUE)
colnames(confusion_matrix) <- c("y=0","y=1")
rownames(confusion_matrix) <- c("yhat=0","yhat=1")
confusion_matrix <- as.table(confusion_matrix)
overall_error <- (confusion_matrix[1,2]+confusion_matrix[2,1])/length(y_test)
true_positive_rate <-  confusion_matrix[2,2] / (confusion_matrix[2,1]+confusion_matrix[2,2])
false_positive_rate <-  confusion_matrix[1,2] / (confusion_matrix[1,1]+confusion_matrix[1,2])
```

&NewLine;

</br>

#### 2) KNN

&NewLine;
</br>  

We first try a grid of k from 1 to 101, calculate their average error rate in 5 trials, and select an optimal k. We select k=61 in this case.


&NewLine;

</br>

```{r, warning=FALSE, message=FALSE}
X = online %>%
  select(n_tokens_title, num_hrefs, num_self_hrefs, num_imgs, average_token_length,
         self_reference_min_shares, is_weekend,avg_negative_polarity,
         data_channel_is_entertainment, data_channel_is_socmed, data_channel_is_world, title_sentiment_polarity)
k_grid = seq(1, 101, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
  out = do(5)*{
    train_ind = sample.int(n, n_train)
    X_train = X[train_ind,]
    X_test = X[-train_ind,]
    y_train = y[train_ind]
    y_test = y[-train_ind]
    
    # scale the train set x
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    
    # scale the test set x using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    # Fit KNN models
    knn_try = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
    # overall error rate
    sum(knn_try != y_test)/n_test
  } 
  
  mean(out$result)
  
}
```
 
```{r}
plot(k_grid, err_grid)
```
  
&NewLine;
</br>  
Then we get the confusion matrix, overall error rate, TPR and FPR by averaging across 100 rounds.
The average overall error rate is around 37.5%, which is better than linear regression model.
  
```{r, warning=FALSE, message=FALSE}
# Get the average accuracy for k=61
repN=30
conf11 <- rep(0,repN)
conf12 <- rep(0,repN)
conf21 <- rep(0,repN)
conf22 <- rep(0,repN)
for (i in 1:repN){
  train_ind = sample.int(n, n_train)
  y_train = y[train_ind]
  y_test = y[-train_ind]
  X_train = X[train_ind,]
  X_test = X[-train_ind,]
  
  # scale the train set x
  scale_factors = apply(X_train, 2, sd)
  X_train_sc = scale(X_train, scale=scale_factors)
  # scale the test set x using the same scale factors
  X_test_sc = scale(X_test, scale=scale_factors)
  
  # Fit KNN models
  knn_yhat = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=61)
  
  # Average confusion matrix
  conf11[i] <- table(y = y_test, yhat = knn_yhat)[1,1]
  conf12[i] <- table(y = y_test, yhat = knn_yhat)[1,2]
  conf21[i] <- table(y = y_test, yhat = knn_yhat)[2,1]
  conf22[i] <- table(y = y_test, yhat = knn_yhat)[2,2]
}
confusion_matrix <- matrix(c(mean(conf11),mean(conf12),mean(conf21),mean(conf22)),ncol=2,byrow=TRUE)
colnames(confusion_matrix) <- c("y=0","y=1")
rownames(confusion_matrix) <- c("yhat=0","yhat=1")
confusion_matrix <- as.table(confusion_matrix)
overall_error <- (confusion_matrix[1,2]+confusion_matrix[2,1])/length(y_test)
true_positive_rate <-  confusion_matrix[2,2] / (confusion_matrix[2,1]+confusion_matrix[2,2])
false_positive_rate <-  confusion_matrix[1,2] / (confusion_matrix[1,1]+confusion_matrix[1,2])
```

&NewLine;

</br>

Confusion matrix:

```{r}
confusion_matrix
```

&NewLine;

</br>

Overall error rate =
```{r}
overall_error
```
&NewLine;

</br>
TPR =
```{r}
true_positive_rate
```
&NewLine;

</br>
FPR =
```{r}
false_positive_rate 
```
&NewLine;

</br>
&NewLine;

</br>

###3. Threshold & classification


&NewLine;

</br>

We first categorize the articals into viral and non-viral using 1400 shares as threshold.
Then we use logit regression to do the classification. We used similar variable selection as linear regression model. The confusion matrix, overall error rate, TPR and FPR averaging across 100 rounds are as below:


&NewLine;

</br>


&NewLine;

</br>

```{r, warning=FALSE, message=FALSE}
online$viral = ifelse(online$shares >= 1400, 1, 0)
X=online
X$shares <- NULL
### 1) Logit regression
X_train = X[train_ind,]
X_test = X[-train_ind,]
glm1 = glm(viral ~ . - weekday_is_sunday - is_weekend, data=X_train, family='binomial')
repN=100
conf11 <- rep(0,repN)
conf12 <- rep(0,repN)
conf21 <- rep(0,repN)
conf22 <- rep(0,repN)
for (i in 1:repN){
  train_ind = sample.int(n, n_train)
  y_train = y[train_ind]
  y_test = y[-train_ind]
  X_train = X[train_ind,]
  X_test = X[-train_ind,]
  
  glm=glm(viral ~ n_tokens_content +num_hrefs+ num_self_hrefs + num_imgs +
            average_token_length + num_keywords + 
            data_channel_is_lifestyle + data_channel_is_entertainment+data_channel_is_bus +data_channel_is_socmed +
            data_channel_is_tech + data_channel_is_world +
            self_reference_min_shares + is_weekend + min_positive_polarity+title_sentiment_polarity, data=X_train, family='binomial')
  phat_test_logit = predict(glm, X_test, type='response')
  yhat_test_logit = ifelse(phat_test_logit > 0.5, 1, 0)
  
  # Average confusion matrix
  conf11[i] <- table(y = y_test, yhat = yhat_test_logit)[1,1]
  conf12[i] <- table(y = y_test, yhat = yhat_test_logit)[1,2]
  conf21[i] <- table(y = y_test, yhat = yhat_test_logit)[2,1]
  conf22[i] <- table(y = y_test, yhat = yhat_test_logit)[2,2]
}
confusion_matrix <- matrix(c(mean(conf11),mean(conf12),mean(conf21),mean(conf22)),ncol=2,byrow=TRUE)
colnames(confusion_matrix) <- c("y=0","y=1")
rownames(confusion_matrix) <- c("yhat=0","yhat=1")
confusion_matrix <- as.table(confusion_matrix)
overall_error <- (confusion_matrix[1,2]+confusion_matrix[2,1])/length(y_test)
true_positive_rate <-  confusion_matrix[2,2] / (confusion_matrix[2,1]+confusion_matrix[2,2])
false_positive_rate <-  confusion_matrix[1,2] / (confusion_matrix[1,1]+confusion_matrix[1,2])
```


&NewLine;

</br>

Confusion matrix:

```{r}
confusion_matrix
```

&NewLine;

</br>

Overall error rate =
```{r}
overall_error
```
&NewLine;

</br>
TPR =
```{r}
true_positive_rate
```
&NewLine;

</br>
FPR =
```{r}
false_positive_rate 
```

&NewLine;

</br>
The average overall error rate is around 36%, which is better than regression & threshold method. The reason might be that by first classifying the y using threshold, the model are less prone to outliers with extreme number of shares.

